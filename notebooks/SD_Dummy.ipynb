{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stance Detection Using Dummy Models\n",
        "\n",
        "`Dummy Models`  \n",
        "`AraStance Dataset`  \n",
        "`Stance Detection` `Arabic Language`\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, we test the performance of dummy models as baselines for the task of article stance detection in the AraStance dataset. The dataset was introduced in the paper:\n",
        "```\n",
        "AraStance: A Multi-Country and Multi-Domain Dataset of Arabic Stance Detection for Fact Checking.\n",
        "```"
      ],
      "metadata": {
        "id": "5H1RrMEJPOXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "ze1WVX-tPTs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from utils import AraStanceData, stance_to_int"
      ],
      "metadata": {
        "id": "NRmuJrPxenqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Raw data"
      ],
      "metadata": {
        "id": "tUQw32wWfBXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Download the raw data:"
      ],
      "metadata": {
        "id": "3yJPBtSZfHXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Tariq60/arastance/archive/refs/heads/main.zip\n",
        "!unzip /content/main.zip"
      ],
      "metadata": {
        "id": "gbOpJdW1fBnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's start by reading the raw data:"
      ],
      "metadata": {
        "id": "T4-u0ol3fKcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train = AraStanceData(\"/content/arastance-main/data/train.jsonl\")\n",
        "raw_val = AraStanceData(\"/content/arastance-main/data/dev.jsonl\")\n",
        "raw_test = AraStanceData(\"/content/arastance-main/data/test.jsonl\")\n",
        "\n",
        "print(f'# training instances: {len(raw_train.stances)}')\n",
        "print(f'# validation instances: {len(raw_val.stances)}')\n",
        "print(f'# testing instances: {len(raw_test.stances)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI2al79OfLXO",
        "outputId": "14ea798d-2fbf-4a6d-d6e2-eaf29154f7da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# training instances: 2848\n",
            "# validation instances: 569\n",
            "# testing instances: 646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's see an instance from the data:"
      ],
      "metadata": {
        "id": "V2gUdXyDfOXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instance_no = 110\n",
        "print(f\"Claim text: {raw_train.claims[raw_train.article_claim[instance_no]]}\")\n",
        "print(f\"Article text: {raw_train.articles[instance_no]}\")\n",
        "print(f\"Stance: {raw_train.stances[instance_no]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gIZNN5KfPcm",
        "outputId": "9d893bc9-1dd7-4668-99ab-bfe8c347b3e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claim text: صدور بيان بقائمة أسماء ممنوعة ووجوب تغيير الاسم خلال ثلاثين يوم.\n",
            "Article text: ماحقيقة الاسماء الممنوعة في السعودية ، سؤال تداوله الجميع عبر مواقع التواصل الاجتماعي بعد أن تناقل الكثيرون قائمة بأسماء منعتها الأحوال المدنية في السعودية شملت الكثير من الأسماء التي قد لايخلو منزل منها ، وتساءل البعض عن أسباب المنع وكيف يمكن تغيير الاسماء التي تم تسميتها مسبقا ؟ الأحوال المدنية توضح حقيقة الاسماء الممنوعة في السعودية أوضحت الأحوال المدنية السعودية أن ما يتم تناقله عبر المواقع الاجتماعية عن أسماء ممنوعة في السعودية هي معلومات خالية من الصحة ، وقالت عبر حسابها الرسمي على تويتر ( ما يتم تداوله حول صدور بيان بقائمة أسماء ممنوعة ووجوب تغيير الاسم خلال ثلاثين يوماً غير صحيح، ونأمل أخذ المعلومات من المصادر الرسمية. كما وأوضح المتحدث الرسمي للأحوال المدنية محمد الجاسر أنه لا يتم تسجيل أسماء المواليد غير الجائزة شرعا والأسماء غير الملائمة اجتماعيا ونحوها، وأضاف أن اللائحة التنفيذية لنظام الأحوال المدنية وضحت أنه لا يجوز التسمية بالأسماء التي نصت الفتاوى الشرعية على عدم جوازها، إضافة الأسماء المكروهة شرعا. وأشار إلى ضرورة التقيد بقواعد اللغة العربية عند تسجيل الأسماء وأن يكون مجردًا من الألقاب فلا تسجل الكلمات التي ليست جزءًا من الاسم، مشيرًا إلى أنه لايتم تسجيل الأسماء المركبة. ولفت إلى أنه في الآونة الأخيرة تناقلت وسائل التواصل الاجتماعي بياناً يحتوي على (50) اسما وأن الأحوال المدنية عممته على المستشفيات لعدم التسجيل بتلك الأسماء، نافيًا بذلك صحة صدور هذا البيان من الأحوال المدنية ، ودعا الجاسر في ختام تصريحه الآباء إلى حسن اختيار الأسماء لأبنائهم متقيدين بالأنظمة والتعليمات المنظمة لذلك.\n",
            "Stance: Disagree\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random classifier"
      ],
      "metadata": {
        "id": "SQLOo3KZen6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let's start with the performance of a random classifier that gives random predictions:"
      ],
      "metadata": {
        "id": "fEndyIsh3sV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_classifier(labels, no_trials = 5):\n",
        "\n",
        "  no_classes = len(np.unique(labels))\n",
        "  accuracy, f1score, mf1score = [], [], []\n",
        "  for _ in range(no_trials):\n",
        "    predictions = np.array([np.random.randint(0, no_classes) for _ in range(len(labels))])\n",
        "\n",
        "    acc = np.sum(predictions == np.array(labels)) / len(labels)\n",
        "    accuracy.append(acc)\n",
        "\n",
        "    f1 = f1_score(labels, predictions, average=None)\n",
        "    f1score.append(f1)\n",
        "\n",
        "    mf1 = f1_score(labels, predictions, average='macro')\n",
        "    mf1score.append(mf1)\n",
        "\n",
        "  return accuracy, f1score, mf1score"
      ],
      "metadata": {
        "id": "kqJ1TE3teW30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_labels = [stance_to_int[stance] for stance in raw_val.stances]\n",
        "val_accuracy, val_f1score, val_mf1score= random_classifier(val_labels)\n",
        "print(\"Validation Resutls:\")\n",
        "print(\"=====================\")\n",
        "print(f\"Mean Accuracy: {np.mean(val_accuracy):.3f}\")\n",
        "agree, disagree, discuss, unrelated = np.mean(val_f1score, axis=0)\n",
        "print(\"Mean Per Class F1 scores:\")\n",
        "print(f\"Agree   : {agree:.3f}\")\n",
        "print(f\"Disagree: {disagree:.3f}\")\n",
        "print(f\"Discuss : {discuss:.3f}\")\n",
        "print(f\"Unrelated: {unrelated:.3f}\")\n",
        "print(f\"Mean Macro F1 scores: {np.mean(val_mf1score):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRde_0unuatQ",
        "outputId": "c079b8c6-aa53-451b-c50a-92f9b8d77ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Resutls:\n",
            "=====================\n",
            "Mean Accuracy: 0.239\n",
            "Mean Per Class F1 scores:\n",
            "Agree   : 0.220\n",
            "Disagree: 0.169\n",
            "Discuss : 0.154\n",
            "Unrelated: 0.327\n",
            "Mean Macro F1 scores: 0.218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = [stance_to_int[stance] for stance in raw_test.stances]\n",
        "test_accuracy, test_f1score, test_mf1score= random_classifier(test_labels)\n",
        "print(\"Testing Resutls:\")\n",
        "print(\"=====================\")\n",
        "print(f\"Mean Accuracy: {np.mean(test_accuracy):.3f}\")\n",
        "agree, disagree, discuss, unrelated = np.mean(test_f1score, axis=0)\n",
        "print(\"Mean Per Class F1 scores:\")\n",
        "print(f\"Agree   : {agree:.3f}\")\n",
        "print(f\"Disagree: {disagree:.3f}\")\n",
        "print(f\"Discuss : {discuss:.3f}\")\n",
        "print(f\"Unrelated: {unrelated:.3f}\")\n",
        "print(f\"Mean Macro F1 scores: {np.mean(test_mf1score):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yDhKoYOeaV_",
        "outputId": "d3255e3a-f0bd-4c59-f647-29298091a01e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Resutls:\n",
            "=====================\n",
            "Mean Accuracy: 0.248\n",
            "Mean Per Class F1 scores:\n",
            "Agree   : 0.230\n",
            "Disagree: 0.156\n",
            "Discuss : 0.160\n",
            "Unrelated: 0.336\n",
            "Mean Macro F1 scores: 0.221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Majority classifier"
      ],
      "metadata": {
        "id": "C6tYSBALgEKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now, let's check the performance of a majority classifier that predicts the majority class all the time:"
      ],
      "metadata": {
        "id": "1NSBnwEM32ZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def majority_classifier(labels, majority_class):\n",
        "\n",
        "  predictions = [stance_to_int[majority_class]] * len(labels)\n",
        "  accuracy = np.sum(np.array(predictions) == np.array(labels)) / len(labels)\n",
        "\n",
        "  f1score = f1_score(labels, predictions, average=None)\n",
        "  mf1score = f1_score(labels, predictions, average='macro')\n",
        "\n",
        "  return accuracy, f1score, mf1score"
      ],
      "metadata": {
        "id": "MXI8hIO7RXdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_labels = [stance_to_int[stance] for stance in raw_val.stances]\n",
        "val_accuracy, val_f1score, val_mf1score= majority_classifier(val_labels, 'unrelated')\n",
        "print(\"Validation Resutls:\")\n",
        "print(\"=====================\")\n",
        "print(f\"Mean Accuracy: {val_accuracy:.3f}\")\n",
        "agree, disagree, discuss, unrelated = val_f1score\n",
        "print(\"Mean Per Class F1 scores:\")\n",
        "print(f\"Agree   : {agree:.3f}\")\n",
        "print(f\"Disagree: {disagree:.3f}\")\n",
        "print(f\"Discuss : {discuss:.3f}\")\n",
        "print(f\"Unrelated: {unrelated:.3f}\")\n",
        "print(f\"Mean Macro F1 scores: {val_mf1score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG_MWeS5w28N",
        "outputId": "03832262-78a3-4d30-ed6a-7e158750c78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Resutls:\n",
            "=====================\n",
            "Mean Accuracy: 0.517\n",
            "Mean Per Class F1 scores:\n",
            "Agree   : 0.000\n",
            "Disagree: 0.000\n",
            "Discuss : 0.000\n",
            "Unrelated: 0.681\n",
            "Mean Macro F1 scores: 0.170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = [stance_to_int[stance] for stance in raw_test.stances]\n",
        "test_accuracy, test_f1score, test_mf1score = majority_classifier(test_labels, 'unrelated')\n",
        "print(\"Testing Resutls:\")\n",
        "print(\"=====================\")\n",
        "print(f\"Mean Accuracy: {test_accuracy:.3f}\")\n",
        "agree, disagree, discuss, unrelated = test_f1score\n",
        "print(\"Mean Per Class F1 scores:\")\n",
        "print(f\"Agree   : {agree:.3f}\")\n",
        "print(f\"Disagree: {disagree:.3f}\")\n",
        "print(f\"Discuss : {discuss:.3f}\")\n",
        "print(f\"Unrelated: {unrelated:.3f}\")\n",
        "print(f\"Mean Macro F1 scores: {test_mf1score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgtSQ6yQgMoT",
        "outputId": "a56bfed0-24d2-46db-f828-227a67ad9beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Resutls:\n",
            "=====================\n",
            "Mean Accuracy: 0.554\n",
            "Mean Per Class F1 scores:\n",
            "Agree   : 0.000\n",
            "Disagree: 0.000\n",
            "Discuss : 0.000\n",
            "Unrelated: 0.713\n",
            "Mean Macro F1 scores: 0.178\n"
          ]
        }
      ]
    }
  ]
}